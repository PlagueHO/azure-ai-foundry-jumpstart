"{\n  \"sop_id\": \"b336e608-1927-4dd6-86f9-ac7fba9c413d\",\n  \"version\": \"1.4\",\n  \"created_at\": \"2025-11-28T02:47:14.471723+00:00\",\n  \"last_updated\": \"2025-11-28T03:15:00+00:00\",\n  \"title\": \"Replace failing storage controller or drive in Azure Stack HCI node (predictive hardware failure)\",\n  \"problem_category\": \"hardware\",\n  \"complexity\": \"medium\",\n  \"system_context\": \"Azure Stack HCI\",\n  \"severity\": \"critical\",\n  \"status\": \"published\",\n  \"approval_level\": \"director\",\n  \"author\": {\n    \"name\": \"Morgan Ellis\",\n    \"email\": \"morgan.ellis.ops@example-corp.internal\"\n  },\n  \"approver\": {\n    \"name\": \"Priya Nambiar\",\n    \"email\": \"priya.nambiar.infrastructure@example-corp.internal\",\n    \"approved_at\": \"2025-11-28T03:00:00+00:00\"\n  },\n  \"problem_description\": \"One or more physical components (storage controller module or disk drive) in an Azure Stack HCI cluster node is reporting a predictive failure or has failed, resulting in degraded storage pool/CSV status, cluster health alerts, or node evacuation/eviction events. This SOP covers isolating the failing component, safely taking the node into maintenance, replacing the hardware, updating firmware/drivers, and returning the node to cluster service while preserving data redundancy.\",\n  \"symptoms\": [\n    \"Cluster health shows 'Warning' or 'Critical' and storage pools are 'Degraded' or 'ReducedAvailability'.\",\n    \"Get-PhysicalDisk reports MediaType/HealthStatus as 'Unhealthy' or 'PredictiveFailure'.\",\n    \"Node hardware management alerts (iDRAC/iLO/IMM) showing predictive failure or component amber LED.\",\n    \"Event logs: Storage or MSFT-SDI events with EventIDs indicating disk/controller errors; repeated CRC/IO timeouts.\",\n    \"VMs on node migrating or being evacuated; automatic resynchronization jobs queued.\"\n  ],\n  \"prerequisites\": [\n    \"Cluster admin credentials (domain\\cluster-admin) with local admin on target node.\",\n    \"Access to vendor hardware management interface (iDRAC, iLO, XClarity) with administrative rights.\",\n    \"Approved maintenance window and change record if required by policy.\",\n    \"Validated, compatible replacement part (same part number or vendor-approved FRU).\",\n    \"Recent full cluster backup and verified snapshot/checkpoint policy as per site rules.\"\n  ],\n  \"required_tools\": [\n    \"Secure console or SSH access to node\",\n    \"Windows Admin Center (WAC) or remote PowerShell (FailoverClusters, Storage modules)\",\n    \"Vendor management portal (iDRAC/iLO) and vendor firmware update tool\",\n    \"Physical toolset for drive/controller replacement (screwdrivers, ESD wrist strap)\",\n    \"Serial/console cable or KVM access if remote management is unreliable\",\n    \"Log collection utility (Get-ClusterLog, Get-StorageDiagnosticInfo script)\"\n  ],\n  \"estimated_resolution_time\": \"60-120 minutes (excluding parts shipping time)\",\n  \"resolution_steps\": [\n    {\n      \"step_number\": 1,\n      \"action\": \"Confirm cluster and component state\",\n      \"details\": \"From a management workstation, connect to the cluster and run: Get-Cluster | Format-List Name,State; Get-ClusterNode | Sort Name,State; Get-StoragePool -IsPrimordial $false | Get-PhysicalDisk | Select FriendlyName,SerialNumber,HealthStatus,OperationalStatus,Usage. Collect output to incident-ticket.\",\n      \"warnings\": \"Do not perform corrective actions before step 2 unless you have immediate vendor guidance.\"\n    },\n    {\n      \"step_number\": 2,\n      \"action\": \"Collect diagnostics and preserve logs\",\n      \"details\": \"Run Get-ClusterLog -Destination C:\\\\Temp\\\\ClusterLogs and export event logs (System, Microsoft-Windows-StorageSpaces-Driver) from the affected node. Use vendor collector (if available) to gather hardware diagnostic package. Attach all files to the incident.\",\n      \"warnings\": \"Log collection can generate large files; ensure adequate disk space and transfer to secure location immediately.\"\n    },\n    {\n      \"step_number\": 3,\n      \"action\": \"Identify and map failing component to physical location\",\n      \"details\": \"Correlate the serial number/FriendlyName from PowerShell to vendor management UI. In iDRAC/iLO/XClarity, verify the drive bay or controller module reporting predictive failure and note the bay ID (e.g., Bay 3), FRU number and LED indicator state.\",\n      \"warnings\": \"Do not remove any component until node is placed in maintenance as described in step 4.\"\n    },\n    {\n      \"step_number\": 4,\n      \"action\": \"Place node into maintenance and evacuate workloads\",\n      \"details\": \"On the cluster manager, place the node in maintenance: Suspend-ClusterNode -Name <NodeName> -Drain. Alternatively use Windows Admin Center 'Maintenance mode'. Confirm role migration: Get-ClusterGroup | Where-Object { $_.OwnerNode -eq '<NodeName>' }.\",\n      \"warnings\": \"Verify that remaining nodes have sufficient capacity before draining. If draining fails, stop and follow 'Troubleshooting' section.\"\n    },\n    {\n      \"step_number\": 5,\n      \"action\": \"Offline the affected physical disk(s) or controller\",\n      \"details\": \"If a single physical disk is affected, set usage to Retired and make it offline: Set-PhysicalDisk -FriendlyName '<name>' -Usage Retired; Repair-VirtualDisk -FriendlyName '<vdisk>' -RemoveMissingPhysicalDisk -Verbose (if required). For controller module replacement, follow vendor guidance to take controller offline and failover any controller-managed caches.\",\n      \"warnings\": \"Do NOT remove a disk that is part of an active resiliency operation unless you have confirmed redundancy and rollback plan.\"\n    },\n    {\n      \"step_number\": 6,\n      \"action\": \"Physically replace the failed component\",\n      \"details\": \"Use ESD protection. Power down the node only if vendor FRU requires it; many hot-swap drive replacements are supported online. Remove the failing drive/controller per vendor FRU procedure and install the replacement. Verify LEDs go to normal state in management UI.\",\n      \"warnings\": \"If hot-swap removing causes additional degrade events, power down immediately following vendor guidance and contact escalation if needed.\"\n    },\n    {\n      \"step_number\": 7,\n      \"action\": \"Update firmware/drivers if required\",\n      \"details\": \"Check replacement part firmware; if not matching cluster baseline, schedule an update. Use vendor utility or Windows Update Catalog offline packages to install driver/firmware updates on the node. Reboot if firmware update requires it and document reboot time.\",\n      \"warnings\": \"Do not interrupt firmware update. Reboots must be coordinated with cluster state and maintenance window.\"\n    },\n    {\n      \"step_number\": 8,\n      \"action\": \"Bring new disk/controller into cluster and initiate resilvering\",\n      \"details\": \"If disk was retired, run Set-PhysicalDisk -FriendlyName '<new>' -Usage AutoSelect and then Repair-VirtualDisk -FriendlyName '<vdisk>' -Verbose to start rebuild. Monitor progress with Get-StorageJob and Get-VirtualDisk -FriendlyName '<vdisk>' | Get-StorageJob.\",\n      \"warnings\": \"Rebuilds can be IO intensive; monitor latency metrics and be prepared to throttle non-critical workloads.\"\n    },\n    {\n      \"step_number\": 9,\n      \"action\": \"Exit maintenance and verify cluster health\",\n      \"details\": \"Resume the node: Resume-ClusterNode -Name <NodeName>. Validate: Get-Cluster | Format-List; Get-ClusterGroup to see failback state; Get-PhysicalDisk to confirm HealthStatus Healthy. Check CSV status: Get-ClusterSharedVolumeState -Name '*' (or use WAC).\",\n      \"warnings\": \"If cluster health does not return to normal, do not resume VM placement; follow Escalation guidance.\"\n    },\n    {\n      \"step_number\": 10,\n      \"action\": \"Document actions and close the incident\",\n      \"details\": \"Update the change record and incident ticket with part FRU, serial numbers replaced, commands run, log attachments, time of resync completion, and any vendor case number. Retain original failed part per retention policy for vendor return.\",\n      \"warnings\": \"Ensure all retained failed parts are tagged and stored in the approved secure area for the vendor RMA window.\"\n    }\n  ],\n  \"verification_steps\": [\n    {\n      \"step\": \"Confirm all nodes report Cluster.CurrentClusterState = 'Running' and no 'Warning' or 'Critical' status via Get-Cluster or WAC.\"\n    },\n    {\n      \"step\": \"Confirm Get-PhysicalDisk shows HealthStatus = 'Healthy' for replaced components and Usage = 'AutoSelect' or expected usage.\"\n    },\n    {\n      \"step\": \"Confirm virtual disk and storage pool resiliency: Get-VirtualDisk | Select FriendlyName,OperationalStatus,HealthStatus; verify 'Healthy' and no ongoing repair jobs.\"\n    },\n    {\n      \"step\": \"Check VM connectivity and I/O: validate latency counters on CSV volumes and test representative VM workloads (application smoke test).\"\n    },\n    {\n      \"step\": \"Confirm vendor management UI shows no predictive alerts and that firmware/drivers match cluster baseline.\"\n    }\n  ],\n  \"troubleshooting\": [\n    {\n      \"issue\": \"Node fails to enter maintenance or Suspend-ClusterNode hangs\",\n      \"solution\": \"Check for pinned cluster groups: Get-ClusterResource | Where-Object { $_.OwnerNode -eq '<NodeName>' -and $_.State -eq 'Online' } and move any resources manually using Move-ClusterGroup. If resource cannot move, inspect dependencies or use Start-ClusterGroup -Name <group> on other node to force owner change, but escalate if persistent.\"\n    },\n    {\n      \"issue\": \"Rebuild/resilver not starting after replacement\",\n      \"solution\": \"Verify that the new disk is recognized: Get-PhysicalDisk -CanPool $true. If disk remains 'Unknown' or 'Not Usable', check RAID metadata and controller passthrough mode. Run vendor diagnostic to clear metadata and reinitialize the disk; if still failing, escalate to vendor hardware support.\"\n    },\n    {\n      \"issue\": \"Replaced part triggers additional warnings or cluster instability\",\n      \"solution\": \"Revert to previous state: if cached controller firmware mismatch suspected, power down and reinsert original component, and open vendor case. Do not attempt multiple firmware cross-flashes without vendor approval.\"\n    },\n    {\n      \"issue\": \"Drive removed but virtual disk shows 'Lost' or 'Failed'\",\n      \"solution\": \"Stop operations and open vendor case immediately. Collect cluster logs and stop automatic repair jobs (Stop-StorageJob -Type Repair) to prevent further damage. Escalate per procedure.\"\n    }\n  ],\n  \"escalation\": {\n    \"condition\": \"Any persistent hardware failure after two attempted replacements, any unexpected data loss indicators, rebuilds failing to progress for >2 hours, or if vendor-level intervention is required.\",\n    \"contact\": \"Primary: Hardware Vendor Support (open a ticket with the FRU serial and diagnostic bundle). Secondary: Internal Infrastructure Lead.\",\n    \"escalation_path\": \"1) Open vendor support case with collected logs and part numbers. 2) Notify Infrastructure Lead via secure messaging and include incident ticket. 3) If vendor response SLA unmet within 2 hours for critical production, escalate to Director of Infrastructure with summary, impact, and request for expedited on-site RMA.\"\n  },\n  \"related_documentation\": [\n    {\n      \"title\": \"Azure Stack HCI - Cluster Maintenance Best Practices (internal)\",\n      \"url\": \"https://intranet.example-corp.internal/docs/azure-stack-hci/cluster-maintenance\"\n    },\n    {\n      \"title\": \"Vendor FRU Replacement Guide (model-AXR-5000)\",\n      \"url\": \"https://vendor-support.example-corp.internal/FRU/AXR-5000/replacement-guide\"\n    },\n    {\n      \"title\": \"Collecting Azure Stack HCI diagnostics and cluster logs\",\n      \"url\": \"https://kb.example-corp.internal/azure-stack-hci/collect-diagnostics\"\n    }\n  ],\n  \"tags\": [\n    \"azure-stack-hci\",\n    \"storage\",\n    \"hardware-replacement\",\n    \"cluster-maintenance\",\n    \"critical\"\n  ],\n  \"version_history\": [\n    {\n      \"version\": \"1.0\",\n      \"date\": \"2025-09-10T10:00:00+00:00\",\n      \"author\": \"Morgan Ellis\",\n      \"changes\": \"Initial draft covering basic drive replacement workflow and validation commands.\"\n    },\n    {\n      \"version\": \"1.2\",\n      \"date\": \"2025-10-22T14:30:00+00:00\",\n      \"author\": \"Morgan Ellis\",\n      \"changes\": \"Added vendor management UI mapping step and expanded troubleshooting scenarios for resilver failures.\"\n    },\n    {\n      \"version\": \"1.4\",\n      \"date\": \"2025-11-28T03:15:00+00:00\",\n      \"author\": \"Morgan Ellis\",\n      \"changes\": \"Refined maintenance mode commands, clarified warnings for firmware updates, added escalation path and verification checklist; approved by director.\"\n    }\n  ]\n}"